{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This notebook uses TF-IDF and machine learning models (Logistic Regression, SVM, Naive Bayes) to predict movie genres\n",
        "# from the combined title and description of movies."
      ],
      "metadata": {
        "id": "Ee8RVXmffnei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "D_CrET7Zfk-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7mXq8i9UX8V",
        "outputId": "b7dc5991-d821-45b4-db85-35eb595994b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Features"
      ],
      "metadata": {
        "id": "QApTQPtQU6wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to lowercase, removes - (Extra space, puntuation)\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  return text"
      ],
      "metadata": {
        "id": "zsWbK6iug7wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Data\n",
        "train_df = pd.read_csv('train_data.csv')\n",
        "test_df = pd.read_csv('test_data_solution.csv')\n",
        "\n",
        "# Applying transformations\n",
        "train_df['Description'] = train_df['Description'].apply(clean_text)\n",
        "train_df['Title'] = train_df['Title'].apply(clean_text)\n",
        "\n",
        "test_df['Description'] = test_df['Description'].apply(clean_text)\n",
        "test_df['Title'] = test_df['Title'].apply(clean_text)"
      ],
      "metadata": {
        "id": "F9--_T0qUtC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "15c88a63-d7b8-4b61-d6f5-ff277a69208f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-42510d27965b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_data_solution.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Convert to lowercase, removes - (Extra space, puntuation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stopwords"
      ],
      "metadata": {
        "id": "cLGZ-y9DHhMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join(word for word in text.split() if word not in stop_words)"
      ],
      "metadata": {
        "id": "LKFQ2b67hASG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_df['Description'] = train_df['Description'].apply(remove_stopwords)\n",
        "test_df['Description'] = test_df['Description'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "dnIOMzAgWfoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Dependent Variable"
      ],
      "metadata": {
        "id": "FHjmRpQaHzjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "y_train = le.fit_transform(train_df['Genre'])\n",
        "y_test = le.transform(test_df['Genre'])"
      ],
      "metadata": {
        "id": "xmf_lztON9wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Features"
      ],
      "metadata": {
        "id": "kR0xNTpvIDQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer # Converts text to no.\n",
        "\n",
        "train_df['text'] = train_df['Title'] + ' ' + train_df['Description']\n",
        "test_df['text'] = test_df['Title'] + ' ' + test_df['Description']\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=20000, # max no. of words\n",
        "    ngram_range=(1, 3), # include single, double, triple words/phrases\n",
        "    min_df=3, # Ignore words that appear less than {mentioned} times\n",
        "    max_df=0.9 # Ignore very common words that appear in more than 90% of data\n",
        ")\n",
        "\n",
        "X_train = tfidf.fit_transform(train_df['text'])\n",
        "X_test = tfidf.transform(test_df['text'])"
      ],
      "metadata": {
        "id": "NXIyyD9jOR_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "JrJaYMGuJVi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'solver': ['lbfgs', 'saga']\n",
        "}\n",
        "lr_model = LogisticRegression(multi_class='multinomial', max_iter=2000, n_jobs=-1)\n",
        "lr_grid_search = GridSearchCV(lr_model, lr_param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "lr_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "lr_best_model = lr_grid_search.best_estimator_\n",
        "print(f\"Best Parameters: {lr_grid_search.best_params_}\")\n",
        "\n",
        "# Predict\n",
        "y_pred_lr = lr_best_model.predict(X_test)\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "# print(classification_report(y_test, y_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gW8da0syjtR",
        "outputId": "f7d49c10-1cd6-4ba3-dde2-9ed556e65554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'solver': 'saga'}\n",
            "Accuracy: 0.5987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine"
      ],
      "metadata": {
        "id": "i3ix44EoK4xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "svm_model = LinearSVC(max_iter=1000)\n",
        "svm_grid_search = GridSearchCV(svm_model, svm_param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "svm_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "svm_best_model = svm_grid_search.best_estimator_\n",
        "print(f\"Best Parameters: {svm_grid_search.best_params_}\")\n",
        "\n",
        "# Predict\n",
        "y_pred_svm = svm_best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"Accuracy: {accuracy_svm:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "# print(classification_report(y_test, y_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blqyV8U3v2qu",
        "outputId": "885e6c1e-ed80-4d06-bf58-b8fb39981267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1}\n",
            "Accuracy: 0.6003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "AMowm5VWLfvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nv_model = MultinomialNB()\n",
        "nv_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_nv = nv_model.predict(X_test)\n",
        "\n",
        "accuracy_nv = accuracy_score(y_test, y_pred_nv)\n",
        "print(f\"Accuracy: {accuracy_nv:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "# print(classification_report(y_test, y_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXL8aumTPWa5",
        "outputId": "f536cf0f-be3a-45aa-95db-276620544b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving Best Model"
      ],
      "metadata": {
        "id": "EeGVkCYRNJrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(svm_best_model, 'svm_genre_prediction_model.pkl')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
        "joblib.dump(le, 'label_encoder.pkl')"
      ],
      "metadata": {
        "id": "dtOjS-NWMx9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80ee2b1-4968-4ddd-c821-99a191b6c4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['label_encoder.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Best Model  (SVM) and make custom prediction"
      ],
      "metadata": {
        "id": "OpMdyziCNNIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = joblib.load('svm_genre_prediction_model.pkl')\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "le = joblib.load('label_encoder.pkl')\n",
        "\n",
        "text = input(\"Enter Title and Description:\")\n",
        "text_cleaned = clean_text(text)\n",
        "text_no_stop = remove_stopwords(text_cleaned)\n",
        "X_input = vectorizer.transform([text_no_stop])\n",
        "pred = model.predict(X_input)\n",
        "genre = le.inverse_transform(pred)\n",
        "print(genre[0])"
      ],
      "metadata": {
        "id": "pTh7_bnJNSFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45808e2e-2dcd-4edf-fb21-9ed3f0f43e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Title and Description:Iron man Since that first suit built in a cave, Tony has created dozens of new suits and upgrades over the years. However, throughout the 50-plus Iron Man models, there are common offensive and defense capabilities found in most iterations.  The primary weapon contained within every suit, the repulsor rays use energy pulses to repel and disrupt enemies and are generated through the suit’s gauntlets. The suit’s booster jets enable Stark to fly fast enough to break the sound barrier, and maneuver more quickly than any fighter jet.  Iron Man’s helmet provides Tony with a heads-up display that gives him 360-degree vision, access to information about his surroundings and enemies, and the ability to transmit and block transmissions along any frequency. The helmet also gives Tony a degree of resistance to EMP and psychic-based attacks.  A weapon centered in Iron Man’s chest, the unibeam is capable of projecting dazzling light, and can also be used as a powerful force beam that is even more powerful than the repulsor ray.  Each of Tony’s suits provides a full range of telecommunications, including the ability to jam and transmit on any frequency, and sophisticated artificial intelligence capable of piloting Tony to safety should he be rendered unconscious.\n",
            " documentary \n"
          ]
        }
      ]
    }
  ]
}